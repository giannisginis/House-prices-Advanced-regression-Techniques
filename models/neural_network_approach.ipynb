{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .csv files\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few houses with more than 4000 sq ft living area that are\n",
    "# outliers, so we drop them from the dataset\n",
    "data=data[data[\"GrLivArea\"] < 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new=data.drop([\"Id\",\"PoolQC\",\"MiscVal\",\"MiscFeature\",\"Fence\",\"FireplaceQu\",\"LotFrontage\",\n",
    "                 \"Alley\",\"GarageYrBlt\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#create simple imputer and use it to fill nan values \n",
    "from sklearn.preprocessing import Imputer\n",
    "median_imputer = Imputer(strategy='median')\n",
    "data_new['MasVnrArea'] = median_imputer.fit_transform(data_new['MasVnrArea'].reshape(-1, 1))\n",
    "#missing values of numerical columns\n",
    "total = data_new.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical features mapping\n",
    "obj_df = data_new.select_dtypes(include=['object']).copy()\n",
    "for i in obj_df:\n",
    "    obj_df[i] = obj_df[i].astype('category')\n",
    "\n",
    "for i in obj_df:\n",
    "    obj_df[i] = obj_df[i].cat.codes\n",
    "#export numerical features\n",
    "numerical_features = data_new.select_dtypes(include=[\"float\",\"int\",\"bool\"]).copy()\n",
    "#concat to new dataframe\n",
    "dataset=pd.concat([numerical_features,obj_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size:\n",
      "(1456, 72) (1456,)\n",
      "\n",
      "Training data size:\n",
      "(1092, 71) (1092,)\n",
      "\n",
      "Test data size:\n",
      "(364, 71) (364,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting up a training and test (validation) set\n",
    "X = dataset.drop(\"SalePrice\", axis=1)\n",
    "y= dataset[\"SalePrice\"]\n",
    "frac_test = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = frac_test, random_state=5)\n",
    "\n",
    "print('Full data size:')\n",
    "print(dataset.shape, data['SalePrice'].shape)\n",
    "print('\\nTraining data size:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('\\nTest data size:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()  # define scaler\n",
    "scaler.fit(X_train)  # fit scaler ONLY on the training data\n",
    "\n",
    "# print('mean: {}\\nstd:  {}'.format(scaler.mean_ , scaler.scale_))\n",
    "\n",
    "# transform on both sets:\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate correlation of features with Sales price\n",
    "corr_concat = pd.concat([X_train,y_train], axis=1)\n",
    "corrmat = corr_concat.corr()\n",
    "corr_list = corrmat['SalePrice'].sort_values(axis=0,ascending=False).iloc[1:]\n",
    "# features with correlation >0.45 and <(-0.45)\n",
    "feat=corr_list[((corr_list.values >0.45)|(corr_list.values < (-0.4))) ].index.tolist()\n",
    "#remove columns of the remaining ones with low correlation among them\n",
    "remove_list = ['1stFlrSF','GarageArea','TotRmsAbvGrd','YearRemodAdd']\n",
    "feat=[ x for x in feat if x not in remove_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size:\n",
      "(1456, 11) (1456,)\n",
      "\n",
      "Training data size:\n",
      "(1092, 11) (1092,)\n",
      "\n",
      "Test data size:\n",
      "(364, 11) (364,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting up again a training and test (validation) set\n",
    "X = dataset[feat]\n",
    "y= dataset[\"SalePrice\"]\n",
    "frac_test = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = frac_test, random_state=5)\n",
    "\n",
    "print('Full data size:')\n",
    "print(dataset[feat].shape, data['SalePrice'].shape)\n",
    "print('\\nTraining data size:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('\\nTest data size:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1092, 11)\n",
      "Epoch 1/50\n",
      "1092/1092 [==============================] - 2s 2ms/step - loss: 16.6428\n",
      "Epoch 2/50\n",
      "1092/1092 [==============================] - 1s 631us/step - loss: 0.0978\n",
      "Epoch 3/50\n",
      "1092/1092 [==============================] - 1s 658us/step - loss: 0.0729\n",
      "Epoch 4/50\n",
      "1092/1092 [==============================] - 1s 699us/step - loss: 0.0663 0s -\n",
      "Epoch 5/50\n",
      "1092/1092 [==============================] - 1s 673us/step - loss: 0.0601\n",
      "Epoch 6/50\n",
      "1092/1092 [==============================] - 1s 669us/step - loss: 0.0568\n",
      "Epoch 7/50\n",
      "1092/1092 [==============================] - 1s 687us/step - loss: 0.0546\n",
      "Epoch 8/50\n",
      "1092/1092 [==============================] - 1s 663us/step - loss: 0.0536\n",
      "Epoch 9/50\n",
      "1092/1092 [==============================] - 1s 669us/step - loss: 0.0525\n",
      "Epoch 10/50\n",
      "1092/1092 [==============================] - 1s 661us/step - loss: 0.0532\n",
      "Epoch 11/50\n",
      "1092/1092 [==============================] - 1s 695us/step - loss: 0.0524\n",
      "Epoch 12/50\n",
      "1092/1092 [==============================] - 1s 676us/step - loss: 0.0531\n",
      "Epoch 13/50\n",
      "1092/1092 [==============================] - 1s 682us/step - loss: 0.0526\n",
      "Epoch 14/50\n",
      "1092/1092 [==============================] - 1s 676us/step - loss: 0.0527\n",
      "Epoch 15/50\n",
      "1092/1092 [==============================] - 1s 674us/step - loss: 0.0526\n",
      "Epoch 16/50\n",
      "1092/1092 [==============================] - 1s 703us/step - loss: 0.0525\n",
      "Epoch 17/50\n",
      "1092/1092 [==============================] - 1s 657us/step - loss: 0.0523\n",
      "Epoch 18/50\n",
      "1092/1092 [==============================] - 1s 684us/step - loss: 0.0525\n",
      "Epoch 19/50\n",
      "1092/1092 [==============================] - 1s 696us/step - loss: 0.0519\n",
      "Epoch 20/50\n",
      "1092/1092 [==============================] - 1s 671us/step - loss: 0.0522\n",
      "Epoch 21/50\n",
      "1092/1092 [==============================] - 1s 691us/step - loss: 0.0521\n",
      "Epoch 22/50\n",
      "1092/1092 [==============================] - 1s 682us/step - loss: 0.0522\n",
      "Epoch 23/50\n",
      "1092/1092 [==============================] - 1s 667us/step - loss: 0.0521\n",
      "Epoch 24/50\n",
      "1092/1092 [==============================] - 1s 673us/step - loss: 0.0519\n",
      "Epoch 25/50\n",
      "1092/1092 [==============================] - 1s 671us/step - loss: 0.0516\n",
      "Epoch 26/50\n",
      "1092/1092 [==============================] - 1s 659us/step - loss: 0.0521\n",
      "Epoch 27/50\n",
      "1092/1092 [==============================] - 1s 683us/step - loss: 0.0522\n",
      "Epoch 28/50\n",
      "1092/1092 [==============================] - 1s 663us/step - loss: 0.0514\n",
      "Epoch 29/50\n",
      "1092/1092 [==============================] - 1s 680us/step - loss: 0.0519 0s - \n",
      "Epoch 30/50\n",
      "1092/1092 [==============================] - 1s 662us/step - loss: 0.0517\n",
      "Epoch 31/50\n",
      "1092/1092 [==============================] - 1s 661us/step - loss: 0.0518\n",
      "Epoch 32/50\n",
      "1092/1092 [==============================] - 1s 701us/step - loss: 0.0523\n",
      "Epoch 33/50\n",
      "1092/1092 [==============================] - 1s 677us/step - loss: 0.0521\n",
      "Epoch 34/50\n",
      "1092/1092 [==============================] - 1s 677us/step - loss: 0.0522\n",
      "Epoch 35/50\n",
      "1092/1092 [==============================] - 1s 740us/step - loss: 0.0520\n",
      "Epoch 36/50\n",
      "1092/1092 [==============================] - 1s 760us/step - loss: 0.0518\n",
      "Epoch 37/50\n",
      "1092/1092 [==============================] - 1s 780us/step - loss: 0.0516\n",
      "Epoch 38/50\n",
      "1092/1092 [==============================] - 1s 754us/step - loss: 0.0513\n",
      "Epoch 39/50\n",
      "1092/1092 [==============================] - 1s 698us/step - loss: 0.0518\n",
      "Epoch 40/50\n",
      "1092/1092 [==============================] - 1s 647us/step - loss: 0.0520\n",
      "Epoch 41/50\n",
      "1092/1092 [==============================] - 1s 637us/step - loss: 0.0516\n",
      "Epoch 42/50\n",
      "1092/1092 [==============================] - 1s 652us/step - loss: 0.0521\n",
      "Epoch 43/50\n",
      "1092/1092 [==============================] - 1s 663us/step - loss: 0.0515\n",
      "Epoch 44/50\n",
      "1092/1092 [==============================] - 1s 631us/step - loss: 0.0515\n",
      "Epoch 45/50\n",
      "1092/1092 [==============================] - 1s 665us/step - loss: 0.0512\n",
      "Epoch 46/50\n",
      "1092/1092 [==============================] - 1s 634us/step - loss: 0.0512\n",
      "Epoch 47/50\n",
      "1092/1092 [==============================] - 1s 667us/step - loss: 0.0516\n",
      "Epoch 48/50\n",
      "1092/1092 [==============================] - 1s 669us/step - loss: 0.0512\n",
      "Epoch 49/50\n",
      "1092/1092 [==============================] - 1s 649us/step - loss: 0.0517\n",
      "Epoch 50/50\n",
      "1092/1092 [==============================] - 1s 651us/step - loss: 0.0505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe07f0ed828>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "# Compile model\n",
    "model.compile(loss= 'mean_squared_logarithmic_error', optimizer=keras.optimizers.Adadelta())\n",
    "\n",
    "feature_cols = X_train\n",
    "labels = y_train\n",
    "print (feature_cols.shape)\n",
    "labels.shape\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 0s 793us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.046024817952906694"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on the test set created by train_test_split\n",
    "model.evaluate(np.array(X_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "import itertools\n",
    "feature_cols_test = X_test\n",
    "labels_test = y_test\n",
    "\n",
    "m = model.predict(np.array(feature_cols_test))\n",
    "predictions = list(itertools.islice(m, y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
